{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ETaXaFEvXULC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Arabic Poem Generator\n",
        "Natural Language Processing (NLP) Course Project  \n",
        "Section: AI  |  Students:\n",
        "* Ahad Alsulami\n",
        "* Raghad Alghamdi\n",
        "* Reouf Alsahafi\n",
        "* Latifah Mohammed"
      ],
      "metadata": {
        "id": "j7J89C98V-db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data and Import Libraries"
      ],
      "metadata": {
        "id": "CaUnS6BCXB_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will read a corpus from the web, as well as importing all the necessary libraries required for our projec."
      ],
      "metadata": {
        "id": "hFkaEc-RrpOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9hDIAJPOHjvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6960a66-85b2-4c28-a287-463cf50319df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size =  282701\n"
          ]
        }
      ],
      "source": [
        "# upload and read data from a web server\n",
        "!wget -q https://raw.githubusercontent.com/zaidalyafeai/ARBML/master/datasets/Poems/poems\n",
        "dataset = open(\"poems\", \"r\").read()\n",
        "print('Dataset size = ', len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "INZxFqjDHvHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d32be7-9fff-42d9-9ad9-4727be8e072f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Dataset Into Words"
      ],
      "metadata": {
        "id": "1CyGIdQpXHT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will use the `word_tokenize` function from `nltk` library to divide the dataset into individual words, which will allow us to manipulate the data more effectively."
      ],
      "metadata": {
        "id": "1ga4ZKijtMgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(dataset)\n",
        "\n",
        "# print the length\n",
        "print(\"The length of the tokens = \",len(tokens))\n",
        "\n",
        "# print the first 5 words\n",
        "print(\"\\nThe first 5 words are: \\n\",tokens[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDxEO1BRH3qQ",
        "outputId": "af97f2ef-ce8f-457a-a02a-a952d91948a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the tokens =  51945\n",
            "\n",
            "The first 5 words are: \n",
            " ['فيا', 'عجبا', 'للناس', 'يستشرفونني', 'كان']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "T6zbDwhyXMPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section outlines the process of building an `n-gram language model`. Firstly, we created a dictionary `ngrams`, where the keys represent individual words and the values are the words that follow them. This allows for multiple values to be added to each key, which affects the probability of text generation.\n",
        "\n",
        "Next, we generate a random sentence by selecting a starting word and looping over the values to find those with a probability above or equal a certain `threshold`. We continue this process until a terminating condition is reached, which is in this case 4 lines and 8 words in each of them."
      ],
      "metadata": {
        "id": "fSTn5NBjucJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary\n",
        "n = 1 # unigram\n",
        "ngrams = {}\n",
        "\n",
        "# loop through all the words in `tokens`\n",
        "# assign values to the keys\n",
        "for i in range(len(tokens)-n):\n",
        "    # form a string of keys\n",
        "    word = ' '.join(tokens[i:i+n])\n",
        "    if word not in ngrams.keys():\n",
        "        ngrams[word] = []\n",
        "    # append allows us to have duplicates in dictionary\n",
        "    ngrams[word].append(tokens[i+n])"
      ],
      "metadata": {
        "id": "S9EUJUOfIEUq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Display the values for the word \\'عجبا\\' :')\n",
        "ngrams['عجبا']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWRSByFxVi3V",
        "outputId": "61c5b66c-4c1a-4a2e-86af-c4a1e20b2324"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Display the values for the word 'عجبا' :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['للناس',\n",
              " 'للناس',\n",
              " 'للناس',\n",
              " 'فقالت',\n",
              " 'للعين',\n",
              " 'مني',\n",
              " 'للعين',\n",
              " 'هذا',\n",
              " 'للعذب',\n",
              " 'أساء']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_poem(start_word):\n",
        "  \"\"\"\n",
        "\n",
        "  This function will generate a poem\n",
        "  from a given start word using unigram language model\n",
        "\n",
        "  - start_word: the first word entered by user\n",
        "\n",
        "  \"\"\"\n",
        "  start = start_word\n",
        "  threshold = random.random() # generate a random threshold\n",
        "  poem = []                   # store generated lines\n",
        "\n",
        "  # enter a new word if its not found\n",
        "  while start not in ngrams.keys():\n",
        "    start_new = input(\"النموذج لا يدعم هذه الكلمة، حاول/ي مرةً أخرى: \")\n",
        "    start = start_new\n",
        "\n",
        "  # for loop to generate 4 lines\n",
        "  for i in range(4):\n",
        "    line = start\n",
        "\n",
        "    # for loop to generate 8 words in each line\n",
        "    for j in range(8):\n",
        "      line_words = nltk.word_tokenize(line) # tokenize lines\n",
        "      start = line_words[-n]     # update with the last current word\n",
        "\n",
        "      candidates = ngrams[start] # list of possible next words\n",
        "\n",
        "      # Compute probabilities of each possible next word based on frequency\n",
        "      probability = [candidates.count(i) / len(candidates)\n",
        "                for i in candidates]\n",
        "\n",
        "      # compute cumulative probabilities for each possible next word\n",
        "      accumulator = [sum(probability[: i + 1])\n",
        "                      for i in range(len(probability))]\n",
        "\n",
        "      # loop over the possible next words and their cumulative probabilities\n",
        "      for word, acc in zip(candidates, accumulator):\n",
        "        if acc >= threshold:\n",
        "          next_word = word\n",
        "          break\n",
        "\n",
        "      line += ' ' + next_word            # add the selected next word to the current line\n",
        "    poem.append(line)                    # add the completed line to the poem list\n",
        "    start = random.choice(ngrams[start]) # new random word for the new line\n",
        "\n",
        "  return '\\n'.join(poem)"
      ],
      "metadata": {
        "id": "y2oVfJeHIG-7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Poem"
      ],
      "metadata": {
        "id": "ETaXaFEvXULC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_word = input(\"أدخل/ي كلمة للبدء: \")\n",
        "poem = generate_poem(start_word)\n",
        "print('\\n', poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neTX2EpUIKUw",
        "outputId": "f717bd96-8460-45d8-a148-d37dc594b5e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "أدخل/ي كلمة للبدء: عهد\n",
            "\n",
            " عهد عاد بنفسجا شكوت صبابتي ووقفت للواشين في كل\n",
            "المنام يزور بكيت إلى أن أرى لقد كنت من\n",
            "أرهب ألا أيها البيت العتيق المحجب لأستمسكن بالود ما\n",
            "ما مضى من هوى صادقا إني بالفتاة لمعجب ألا\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n"
      ],
      "metadata": {
        "id": "1OqPUdee81Ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset (Corpus):**\n",
        "* [ARBML GitHub Repository](https://github.com/ARBML/ARBML)\n",
        "\n",
        "**Text generating:**\n",
        "* [Text Generation Using N-Gram ](https://medium.com/@vsagziyagli/text-generation-using-n-grams-ef49e6e43d39)   \n",
        "* CCAI-413: Natural Language Processing | Lab#3 N-Gram Language Models"
      ],
      "metadata": {
        "id": "BI5v3QLT87pC"
      }
    }
  ]
}
